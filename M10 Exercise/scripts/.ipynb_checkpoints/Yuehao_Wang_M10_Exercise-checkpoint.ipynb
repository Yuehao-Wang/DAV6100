{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Anaconda as Administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#Author: Yuehao Wang\n",
    "#Version: 1.0\n",
    "#Title: Email script with MD5 Checksum File Exchange\n",
    "#NOTE: you will need to prepare this file for import\n",
    "#-------------------------------------------------\n",
    "\n",
    "#import libraries\n",
    "import email\n",
    "import smtplib\n",
    "import ssl\n",
    "import xlrd\n",
    "import xlwt\n",
    "import s3fs\n",
    "import hashlib #check sum\n",
    "import shutil#zip\n",
    "import time#timestamps\n",
    "import boto3#s3 access\n",
    "import os #used to make a directory for save our files\n",
    "import re #regex library\n",
    "import mysql.connector\n",
    "\n",
    "from mysql.connector import errorcode\n",
    "from shutil import make_archive\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database.\n",
      "['DAILY_DIMENSIONS_20210403232819', 'DAILY_DIMENSIONS_20210403233117', 'DAILY_DIMENSIONS_20210403234759', 'date_dim_20210307194630.csv', 'date_dim_20210307195316.csv', 'date_dim_20210307195332.csv', 'date_dim_20210307203403.csv', 'date_dim_20210307225115.csv', 'date_dim_20210308225117.csv', 'date_dim_20210309225117.csv', 'date_dim_20210310225117.csv', 'date_dim_20210311225117.csv', 'date_dim_20210403223217.csv', 'date_dim_20210403224445.csv', 'date_dim_20210403225146.csv', 'date_dim_20210403225217.csv', 'date_dim_20210403225943.csv', 'date_dim_20210403232819.csv', 'date_dim_20210403233117.csv', 'date_dim_20210403234759.csv', 'date_dim_20210403235103.csv', 'fact_orders_20210307203403.csv', 'fact_orders_20210307225115.csv', 'fact_orders_20210308225117.csv', 'fact_orders_20210309225117.csv', 'fact_orders_20210310225117.csv', 'fact_orders_20210311225117.csv', 'orders_dim_20210307225115.csv', 'orders_dim_20210308225117.csv', 'orders_dim_20210309225117.csv', 'orders_dim_20210310225117.csv', 'orders_dim_20210311225117.csv', 'order_item_dim_20210307195316.csv', 'order_item_dim_20210307195332.csv', 'order_item_dim_20210307203403.csv', 'product_dim_20210307194630.csv', 'product_dim_20210307195316.csv', 'product_dim_20210307195332.csv', 'product_dim_20210307203403.csv', 'product_dim_20210307225115.csv', 'product_dim_20210308225117.csv', 'product_dim_20210309225117.csv', 'product_dim_20210310225117.csv', 'product_dim_20210311225117.csv', 'supplier_dim_20210307194630.csv', 'supplier_dim_20210307195316.csv', 'supplier_dim_20210307195332.csv', 'supplier_dim_20210307203403.csv', 'supplier_dim_20210307225115.csv', 'supplier_dim_20210308225117.csv', 'supplier_dim_20210309225117.csv', 'supplier_dim_20210310225117.csv', 'supplier_dim_20210311225117.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'date_dim_20210403235103.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1. Generate Dimension Files. Steps below. In this example, we will generate just one file. \n",
    "\n",
    "'''identify location of the dimension files. This is up to you, but this is the location \n",
    "where your OUTFILE function in MySQL will post the files'''\n",
    "output_file_location = 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads'\n",
    "\n",
    "# connect to server\n",
    "connection = mysql.connector.connect(user='root', password='123456',host='localhost', port ='3306')\n",
    "print('Connected to database.')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#run the sql file \n",
    "sql_file = \"./dimension_output_example.sql\"\n",
    "\n",
    "with open(sql_file) as f:\n",
    "    cursor.execute(f.read(), multi=True)\n",
    "    \n",
    "#print the results    \n",
    "result = os.listdir(output_file_location)\n",
    "print(result)\n",
    "\n",
    "'''now that you have run your file, you will need to search for it in the folder. \n",
    "The following code is a series of regular experessions that allows you to search for the latest file \n",
    "in a directory based on the timestamp. Then it provides the latest file as the output.'''\n",
    "\n",
    "\n",
    "#example of the file name date_dim_20200725101425.csv\n",
    "files_in_directory = os.listdir(output_file_location)\n",
    "file_regex = re.compile(r'date_dim_\\d{14}.csv$')#regex search for date_dimension file with 14 digits\n",
    "filtered_files = [ x for x in files_in_directory if file_regex.match(x)]\n",
    "sorted_files = sorted(filtered_files,reverse=True)\n",
    "sorted_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210403235103\n",
      "C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/DAILY_DIMENSIONS_20210403235103\n"
     ]
    }
   ],
   "source": [
    "#Step 2. Move Files to folder 'checksum'\n",
    "executiontime = str(re.findall('\\d+', sorted_files[0])).replace('[\\'','').replace('\\']','')\n",
    "print(executiontime)\n",
    "\n",
    "directory_name = 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/'+'DAILY_DIMENSIONS_%s'%(executiontime)\n",
    "print(directory_name)\n",
    "\n",
    "os.mkdir(directory_name) #creates folder structure called 'checksum'\n",
    "  \n",
    "os.rename(output_file_location+'/'+sorted_files[0], directory_name+'/'+sorted_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date_dim_20210403235103.csv', 'md5_checksum.txt']\n"
     ]
    }
   ],
   "source": [
    "#Step 3. Create check sum procedure\n",
    "#create md5 checksum file .txt\n",
    "def file_as_bytes(file):\n",
    "    with file:\n",
    "        return file.read()\n",
    "\n",
    "md5filecontent = hashlib.md5(file_as_bytes(open(directory_name+'/'+sorted_files[0], 'rb'))).hexdigest()\n",
    "f_name = directory_name+'/'+'md5_checksum.txt'\n",
    "\n",
    "f = open(f_name, \"a\")\n",
    "f.write(md5filecontent)\n",
    "f.close()\n",
    "\n",
    "print (os.listdir(directory_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/DAILY_DIMENSIONS_20210403235103\n",
      "DAILY_DIMENSIONS_20210403235103.zip\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Zip files with checksum in directory called checksum. \n",
    "zipName = directory_name\n",
    "print(zipName)\n",
    "make_archive(zipName, 'zip', root_dir=directory_name)\n",
    "\n",
    "file_regex_zip = re.compile(r'DAILY_DIMENSIONS_\\d{14}.zip$')\n",
    "files_in_directory_zip = os.listdir(r'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads')\n",
    "filtered_files_zip = [ x for x in files_in_directory_zip if file_regex_zip.match(x)]\n",
    "sorted_files_zip = sorted(filtered_files_zip,reverse=True)\n",
    "zipFileName = sorted_files_zip[0]\n",
    "\n",
    "\n",
    "print(zipFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/DAILY_DIMENSIONS_20210403235103.zip\n",
      "C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/DAILY_DIMENSIONS_20210403235103.zip\n",
      "yw-m10-exercise-update-bucket\n",
      "DAILY_DIMENSIONS_20210403235103.zip\n",
      "Successfull uploaded file to location: s3/yw-m10-exercise-update-bucket/DAILY_DIMENSIONS_20210403235103.zip\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Load zipped package into AWS S3 with checksum.\n",
    "s3pathName = 'yw-m10-exercise-update-bucket'  #specify name of your s3 bucket\n",
    "zipFileNameFullPath = directory_name+'.zip'\n",
    "print(zipFileNameFullPath)\n",
    "\n",
    "#print inputs for s3\n",
    "print(zipFileNameFullPath)\n",
    "print(s3pathName)\n",
    "print(zipFileName)\n",
    "\n",
    "#connect and load the file to s3\n",
    "s3Resource = boto3.resource('s3')\n",
    "s3Resource.meta.client.upload_file(zipFileNameFullPath, s3pathName, zipFileName)\n",
    "\n",
    "#print success message\n",
    "print(\"Successfull uploaded file to location: \"+str('s3/%s/%s'%(s3pathName,zipFileName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
